# -*- coding: utf-8 -*-
"""informe_francis (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kGJG7xejs0Ug8wyXPQYxLNZJJQ3SVFDb
"""

#!pip install pandas numpy psycopg2-binary gspread google-auth requests

import pandas as pd
import re
import psycopg2
import gspread
from google.auth.transport.requests import Request
from google.oauth2.service_account import Credentials
import requests
import numpy as np

# Configuración de la conexión
db_config = {
    "dbname": "dobetter_unilever",
    "user": "readonly_user",
    "password": "secure_password",
    "host": "db-prod-v16-instance-1.ckpl9rpe7fu4.sa-east-1.rds.amazonaws.com",
    "port": 5432
}

try:
    # Conexión a la base de datos
    connection = psycopg2.connect(**db_config)
    cursor = connection.cursor()

    # Ejecutar una consulta de prueba
    cursor.execute("SELECT version();")
    db_version = cursor.fetchone()
    print(f"Conectado a la base de datos:\n{db_version}")

    # Cierra el cursor y la conexión
    cursor.close()
    connection.close()
    print("Conexión cerrada correctamente.")

except Exception as e:
    print(f"Error al conectar a la base de datos: {e}")

# Query con parámetro seguro
query = """
SELECT
    dt.id AS id_dt,
    dt.dt_number,
    invoice.id AS invoice_id,
    invoice.legal_number,
    invoice.submitter AS cliente,
    invoice_log.description,
    invoice_log.updated_by,
    invoice_log.invoice_status_id as "status",
    CASE invoice.updated_by
        WHEN 312 THEN 'Diego Rios'
        WHEN 3   THEN 'Francis Perez'
        WHEN 322 THEN 'Luis Zegers'
        WHEN 316 THEN 'Franco Castro'
    END AS updated_by_name,
    (invoice_log.its AT TIME ZONE 'UTC' AT TIME ZONE 'America/Santiago') AS "Fecha",
    invoice.pod_date
FROM dobetter_unilever.dt
JOIN dobetter_unilever.invoice
    ON dt.id = invoice.dt_id
JOIN dobetter_unilever.invoice_log
    ON invoice.id = invoice_log.invoice_id
  --WHERE invoice.updated_by IN (312, 316, 322, 3)
  -- AND invoice_log.invoice_status_id <> 5
  --AND invoice_log.description IS NOT NULL
  --AND invoice_log.description <> ''
  WHERE invoice_log.its > NOW() - INTERVAL '2 months'
ORDER BY invoice_log.its DESC;
"""

try:
    # Conectar a la base de datos
    connection = psycopg2.connect(**db_config)

    # Leer datos en un DataFrame con parámetros seguros
    dbs = pd.read_sql_query(query, connection)

    # Imprimir las primeras filas para verificar
    print(dbs.head(1))

    # Cerrar conexión
    connection.close()
    print("Conexión cerrada correctamente.")

except Exception as e:
    print(f"Error al ejecutar la consulta: {e}")

dbs

# dbs = resultado de la query

#Hacer filtros comentados en la query
# 1️⃣ Filtrar solo los updated_by específicos
df_filtro1 = dbs[dbs["updated_by"].isin([312, 316, 322, 3])]

# 2️⃣ Excluir status = 5
df_filtro2 = df_filtro1[df_filtro1["status"] != 5]

# 3️⃣ Quitar descripciones nulas
df_filtro3 = df_filtro2[df_filtro2["description"].notna()]

# 4️⃣ Quitar descripciones vacías
df_final = df_filtro3[df_filtro3["description"] != ""]

# Regex con grupo de captura
pattern = r"(\(\(.*?\)\))"

# Crear nueva columna solo con comentario objetivo
df_final["desc_limpia"] = df_final["description"].str.extract(pattern)

# Filtrar filas que tienen comentario
df_filtrado = df_final[df_final["desc_limpia"].notna()]

df_filtrado

# 1️⃣ Obtener lista de FT únicos del df filtrado
ft_filtrados = df_filtrado["legal_number"].unique()

# 2️⃣ Filtrar en dbs solo las filas que correspondan a esos FT
df_check = dbs[dbs["legal_number"].isin(ft_filtrados)]

# 3️⃣ Encontrar legal_number que tienen status>2 en el df original
dt_con_status = df_check[df_check["status"] > 2]["legal_number"].unique()

# 4️⃣ Mantener solo los dt_number que NO tienen status>2
df_filtrado_final = df_filtrado[~df_filtrado["legal_number"].isin(dt_con_status)]

# Resultado
df_filtrado_final

#Registrar Status 1 (no presentado)
# 1. Filtrar solo las filas con status = 1
solo_1 = dbs[dbs["status"] == 1]

# 2. Buscar los legal_number que en el df original tengan status distintos a 1
con_otro_status = dbs.loc[dbs["status"] != 1, "legal_number"].unique()

# 3. Quitar esos legal_number de nuestro filtrado
no_presentados = solo_1[~solo_1["legal_number"].isin(con_otro_status)]

# 4. Asignar description
no_presentados["description"] = "((R0,F0,S0,NO PRESENTADO-SIN RESPONSABLE))"

#Para las que están listas para procesar por Francis
no_presentados.loc[no_presentados["pod_date"].notna(), "description"] = "((R0,F0,S0,CONFORMACION PENDIENTE-CAJA))"

# Para CENCOSUD
no_presentados.loc[
    no_presentados["cliente"] == "CENCOSUD RETAIL S A", "description"
] = "((R0,F0,S0,CENCOSUD-Caja))"

# Para WALMART
no_presentados.loc[
    no_presentados["cliente"] == "WALMART CHILE S A", "description"
] = "((R0,F0,S0,WALMART-Caja))"
no_presentados

#Registrar Status 3 (atados)
# 1. Filtrar solo las filas con status = 3
solo_3 = dbs[dbs["status"] == 3]

# 2. Buscar los legal_number que en el df original tengan status distintos a 3
con_otro_status = dbs.loc[dbs["status"] > 3, "legal_number"].unique()

# 3. Quitar esos legal_number de nuestro filtrado
atadas = solo_3[~solo_3["legal_number"].isin(con_otro_status)]

# 4. Asignar description
atadas["description"] = "((R0,F0,S0,ATADA-TTE UL Caja))"

atadas

df_final_total = pd.concat([df_filtrado_final, no_presentados, atadas], ignore_index=True)
# Crear nueva columna solo con comentario objetivo
df_final_total["desc_limpia"] = df_final_total["description"].str.extract(pattern)

df_final_total["updated_by"] = df_final_total["updated_by"].fillna(0)

#Ve los NaN
df_final_total[df_final_total.isna().any(axis=1)]

# Ordenar por dt_number, legal_number y Fecha descendente (más reciente primero)
df_final_total = df_final_total.sort_values(["dt_number", "legal_number", "Fecha"], ascending=[True, True, False])

# Mantener solo la primera fila de cada combinación dt_number + legal_number
df_final_total = df_final_total.drop_duplicates(subset=["dt_number", "legal_number"], keep="first").reset_index(drop=True)
df_final_total

# Copiar DataFrame para no modificar el original
df = df_final_total.copy()

# Convertir desc_limpia a mayúsculas
df['desc_limpia'] = df['desc_limpia'].str.upper()

# Patrón esperado: ((R<numero>,F<numero>,S<numero>,<motivo>))
pattern = r"^\(\(R\d+,F\d+,S\d+,.+\)\)$"

# Máscara para filas válidas
mask = df['desc_limpia'].str.match(pattern)

# Filas válidas e inválidas
df_validos = df[mask].copy()
df_invalidos = df[~mask].copy()

print("Filas válidas:", df_validos.shape[0])
print("Filas inválidas:", df_invalidos.shape[0])

# Si quieres, puedes guardar las inválidas en un CSV para revisión
df_invalidos.to_csv("filas_invalidas.csv", index=False)
df_filtrado_final = df_validos

#Pasamos todo a mayus para evitar errores:
df_filtrado_final['desc_limpia'] = df_filtrado_final['desc_limpia'].str.upper()

# Seleccionamos solo las columnas que necesitamos
df = df_filtrado_final[['dt_number','legal_number','status','updated_by_name','Fecha','desc_limpia']].copy()

# Función para extraer los datos de desc_limpia
def parse_desc(desc):
    # Quitar los doble paréntesis
    desc = desc.strip("()")
    # Separar por comas (R...,F...,S...,motivo)
    parts = desc.split(",", 3)  # máximo 4 partes
    # Extraer valores numéricos de R, F, S
    rechazados = int(parts[0].replace("R",""))
    faltantes  = int(parts[1].replace("F",""))
    sobrantes  = int(parts[2].replace("S",""))
    motivo     = parts[3]
    return pd.Series([rechazados, faltantes, sobrantes, motivo])

# Aplicar la función y crear nuevas columnas
df[['rechazados','faltantes','sobrantes','motivo']] = df['desc_limpia'].apply(parse_desc)

df_filtrado_final = df
# Opcional: ver resultado
df_filtrado_final.head()

url = "https://drive.google.com/uc?export=download&id=1Ujb0dONC2U3E-fEgHKQoHg849yOBTGea"
r = requests.get(url)
with open("credenciales.json", "wb") as f:
    f.write(r.content)

print("JSON descargado correctamente")

# Autenticación con credenciales de servicio
scope = ["https://www.googleapis.com/auth/spreadsheets"]
creds = Credentials.from_service_account_file("credenciales.json", scopes=scope)

client = gspread.authorize(creds)

# ID de tu Google Sheet
SHEET_ID = "10xSoygvM4fVkq7f8w6-eoyfxnbspNV4Wjg3kKMfw6vI"
sheet = client.open_by_key(SHEET_ID).sheet1

# Limpia la hoja antes
sheet.clear()

# Copiar DataFrame
df_to_upload = df_filtrado_final.copy()

# Convertir datetime a string
for col in df_to_upload.select_dtypes(include=['datetime64[ns]', 'datetime64[ns, UTC]']).columns:
    df_to_upload[col] = df_to_upload[col].astype(str)

# Reemplazar NaN por ""
df_to_upload = df_to_upload.replace({np.nan: ""})

# Reemplazar inf / -inf por valores grandes o vacío
df_to_upload = df_to_upload.replace([np.inf, -np.inf], "")

# Limpiar hoja y subir
sheet.clear()
sheet.update([df_to_upload.columns.values.tolist()] + df_to_upload.values.tolist())
