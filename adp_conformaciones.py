# -*- coding: utf-8 -*-
"""informe_francis (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kGJG7xejs0Ug8wyXPQYxLNZJJQ3SVFDb
"""

#!pip install pandas numpy psycopg2-binary gspread google-auth requests

import pandas as pd
import re
import psycopg2
import gspread
from google.auth.transport.requests import Request
from google.oauth2.service_account import Credentials
import requests
import numpy as np

# Configuración de la conexión
db_config = {
    "dbname": "dobetter_unilever",
    "user": "readonly_user",
    "password": "secure_password",
    "host": "db-prod-v16-instance-1.ckpl9rpe7fu4.sa-east-1.rds.amazonaws.com",
    "port": 5432
}

try:
    # Conexión a la base de datos
    connection = psycopg2.connect(**db_config)
    cursor = connection.cursor()

    # Ejecutar una consulta de prueba
    cursor.execute("SELECT version();")
    db_version = cursor.fetchone()
    print(f"Conectado a la base de datos:\n{db_version}")

    # Cierra el cursor y la conexión
    cursor.close()
    connection.close()
    print("Conexión cerrada correctamente.")

except Exception as e:
    print(f"Error al conectar a la base de datos: {e}")

# Query con parámetro seguro
query = """
SELECT
    dt.id AS id_dt,
    dt.dt_number,
    invoice.id AS invoice_id,
    invoice.legal_number,
    invoice_log.description,
    invoice_log.updated_by,
    invoice_log.invoice_status_id as "status",
    CASE invoice.updated_by
        WHEN 312 THEN 'Diego Rios'
        WHEN 3   THEN 'Francis Perez'
        WHEN 322 THEN 'Luis Zegers'
        WHEN 316 THEN 'Franco Castro'
    END AS updated_by_name,
    (invoice_log.its AT TIME ZONE 'UTC' AT TIME ZONE 'America/Santiago') AS "Fecha"
FROM dobetter_unilever.dt
JOIN dobetter_unilever.invoice
    ON dt.id = invoice.dt_id
JOIN dobetter_unilever.invoice_log
    ON invoice.id = invoice_log.invoice_id
WHERE invoice.updated_by IN (312, 316, 322, 3)
  -- AND invoice_log.invoice_status_id <> 5
  AND invoice_log.description IS NOT NULL
  AND invoice_log.description <> ''
  AND invoice_log.its > '2024-01-01'
ORDER BY invoice_log.its DESC;
"""

try:
    # Conectar a la base de datos
    connection = psycopg2.connect(**db_config)

    # Leer datos en un DataFrame con parámetros seguros
    dbs = pd.read_sql_query(query, connection)

    # Imprimir las primeras filas para verificar
    print(dbs.head(1))

    # Cerrar conexión
    connection.close()
    print("Conexión cerrada correctamente.")

except Exception as e:
    print(f"Error al ejecutar la consulta: {e}")

dbs

# Regex con grupo de captura
pattern = r"(\(\(.*?\)\))"

# Crear nueva columna solo con comentario objetivo
dbs["desc_limpia"] = dbs["description"].str.extract(pattern)

# Filtrar filas que tienen comentario
df_filtrado = dbs[dbs["desc_limpia"].notna()]

df_filtrado

# 1️⃣ Obtener lista de dt_number únicos del df filtrado
dt_numbers_filtrados = df_filtrado["dt_number"].unique()

# 2️⃣ Filtrar en dbs solo las filas que correspondan a esos dt_number
df_check = dbs[dbs["dt_number"].isin(dt_numbers_filtrados)]

# 3️⃣ Encontrar dt_number que tienen status=5 en el df original
dt_con_status_5 = df_check[df_check["status"] == 5]["dt_number"].unique()

# 4️⃣ Mantener solo los dt_number que NO tienen status=5
df_filtrado_final = df_filtrado[~df_filtrado["dt_number"].isin(dt_con_status_5)]

# Resultado
df_filtrado_final

# Copiar DataFrame para no modificar el original
df = df_filtrado_final.copy()

# Convertir desc_limpia a mayúsculas
df['desc_limpia'] = df['desc_limpia'].str.upper()

# Patrón esperado: ((R<numero>,F<numero>,S<numero>,<motivo>))
pattern = r"^\(\(R\d+,F\d+,S\d+,.+\)\)$"

# Máscara para filas válidas
mask = df['desc_limpia'].str.match(pattern)

# Filas válidas e inválidas
df_validos = df[mask].copy()
df_invalidos = df[~mask].copy()

print("Filas válidas:", df_validos.shape[0])
print("Filas inválidas:", df_invalidos.shape[0])

# Si quieres, puedes guardar las inválidas en un CSV para revisión
df_invalidos.to_csv("filas_invalidas.csv", index=False)
df_filtrado_final = df_validos

#Pasamos todo a mayus para evitar errores:
df_filtrado_final['desc_limpia'] = df_filtrado_final['desc_limpia'].str.upper()

# Seleccionamos solo las columnas que necesitamos
df = df_filtrado_final[['dt_number','legal_number','status','updated_by_name','Fecha','desc_limpia']].copy()

# Función para extraer los datos de desc_limpia
def parse_desc(desc):
    # Quitar los doble paréntesis
    desc = desc.strip("()")
    # Separar por comas (R...,F...,S...,motivo)
    parts = desc.split(",", 3)  # máximo 4 partes
    # Extraer valores numéricos de R, F, S
    rechazados = int(parts[0].replace("R",""))
    faltantes  = int(parts[1].replace("F",""))
    sobrantes  = int(parts[2].replace("S",""))
    motivo     = parts[3]
    return pd.Series([rechazados, faltantes, sobrantes, motivo])

# Aplicar la función y crear nuevas columnas
df[['rechazados','faltantes','sobrantes','motivo']] = df['desc_limpia'].apply(parse_desc)

df_filtrado_final = df
# Opcional: ver resultado
print(df_filtrado_final.head())

url = "https://drive.google.com/uc?export=download&id=1Ujb0dONC2U3E-fEgHKQoHg849yOBTGea"
r = requests.get(url)
with open("credenciales.json", "wb") as f:
    f.write(r.content)

print("JSON descargado correctamente")

# Autenticación con credenciales de servicio
scope = ["https://www.googleapis.com/auth/spreadsheets"]
creds = Credentials.from_service_account_file("credenciales.json", scopes=scope)

client = gspread.authorize(creds)

# ID de tu Google Sheet
SHEET_ID = "10xSoygvM4fVkq7f8w6-eoyfxnbspNV4Wjg3kKMfw6vI"
sheet = client.open_by_key(SHEET_ID).sheet1

# Limpia la hoja antes
sheet.clear()

# Copiar DataFrame
df_to_upload = df_filtrado_final.copy()

# Convertir datetime a string
for col in df_to_upload.select_dtypes(include=['datetime64[ns]', 'datetime64[ns, UTC]']).columns:
    df_to_upload[col] = df_to_upload[col].astype(str)

# Reemplazar NaN por ""
df_to_upload = df_to_upload.replace({np.nan: ""})

# Reemplazar inf / -inf por valores grandes o vacío
df_to_upload = df_to_upload.replace([np.inf, -np.inf], "")

# Limpiar hoja y subir
sheet.clear()
sheet.update([df_to_upload.columns.values.tolist()] + df_to_upload.values.tolist())